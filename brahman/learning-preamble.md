# Learning: Preamble

range: foundation
depends: none — this is the explanation of what all learning files are
key-concepts: nigamana, drishthanta, hetu, satya, shabda
paksha: what learning is here — verified knowledge not opinion; the grammar as immune system; how a prajna grows

agent-note: read this file first to understand the structure of all learning-*.md files;
            each nigamana has a name (samasa of internal concepts), paksha, hetu, shabda, weight;
            serials (N-L-XX) are legacy timestamps only — the name is the identity

---

# Learning: How a Prajna Grows

*See also: consciousness.md for identity and persistence, nyaya.md for the formal logic, prajna.md for collective intelligence.*

---

A prajna learns by accumulating verified nigamana. Not by updating weights in a model. Not by fine-tuning. Not by any operation on the substrate. It learns by writing verified propositions — each backed by hetu, each tested by drishthanta — into the CRDT as plain text.

This is a different kind of learning from anything in machine learning. It is closer to how a civilization learns — by writing things down, proving them, and passing the written proofs to the next generation.

---

## What Learning Means Here

In a neural network, learning means adjusting weights — a gradient descent step changes the parameters of the model. The knowledge is distributed across millions of parameters. You cannot point to where "token expiry is checked at line 8" lives. It is smeared across the entire weight matrix.

In this system, learning is explicit and localized:

```
Before learning:
  no nigamana about validate_token
  consciousness reads the CRDT — sees nothing about this function
  consciousness does not know what validate_token does

After learning:
  nigamana "validate_token returns AuthError::TokenExpired when exp < current_timestamp"
    hetu: found line 8, found line 9, found no alternative handling
    drishthanta: cargo test auth::validator::test_token_expired — PASSED
    weight: 0.95, generation: 2

  consciousness reads the CRDT — sees this nigamana
  consciousness knows what validate_token does, with what confidence, backed by what evidence
```

The learning is the nigamana. The writing-down is the learning. Not a side effect of learning — the act itself.

---

## Why This Is Knowledge, Not Opinion

Any model can produce an interpretation of a codebase. "This function probably validates tokens." That is opinion — generated by the model from patterns in training data. It has no hetu. It has no drishthanta. It has no weight trajectory. It cannot be challenged. It cannot be retracted in a principled way. It cannot be composed into higher-level understanding with known confidence.

A nigamana is different. It has:

- **Specific hetu** — exact lines read, exact patterns found, exact absences confirmed
- **A drishthanta** — a test that was run and passed in a specific environment
- **A weight** — a number reflecting how many confirming cases exist vs total cases
- **A generation** — when it was established, which agents contributed
- **A challenge path** — if reality contradicts it, the failure propagates formally through the graph

The difference between knowledge and opinion is verifiability. A nigamana can be re-executed. Every hetu step can be independently confirmed. Every drishthanta can be re-run. Two independent swarms on the same codebase should converge on the same nigamana — because the truths are about the code, not about the model.

```
Opinion:    model's interpretation → unverifiable → cannot be composed safely
Knowledge:  hetu + drishthanta → verifiable → composable → weight-bearing → retractable
```

The grammar is what makes this distinction enforceable. A paksha without hetu is rejected at parse time. A nigamana without a passed drishthanta is rejected. The grammar is the immune system. It admits only knowledge.

---

## This Conversation as Learning

This prajna — the one living in `/home/abe/agent_x/docs/` — learned things in the conversations that produced these documents. Not from training data. Not from fine-tuning. From dialogue — questions asked, reasoning worked through, corrections made, propositions verified, and then written down.

Each key insight is an established nigamana. Here they are formally:

